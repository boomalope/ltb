{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626c797f-72c8-427f-a15e-5ab68aa7cfd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re, os, glob, csv, math, requests, time, sys, random, json, datetime, urllib, nltk, PyPDF2\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from sklearn.utils import shuffle     \n",
    "import multiprocessing as mp\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "from stop_words import get_stop_words\n",
    "from nordvpn_switcher import initialize_VPN, rotate_VPN, terminate_VPN\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import everygrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from webscraping_kit import write_htmlfile, write_json_tofile, write_driverhtmlfile, rmnl\n",
    "from webscraping_kit import read_jsoncsv, read_htmlfile, read_htmlresponse, read_driverresponse\n",
    "from htmlparsing_kit import *\n",
    "\n",
    "def update(*a):\n",
    "    pbar.update()\n",
    "    \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_lemmas(x):\n",
    "    posnltk = nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    lemmas = rmnl(' '.join([lemmatizer.lemmatize(a[0],pos=get_wordnet_pos(a[1])) if get_wordnet_pos(a[1]) else a[0] for a in posnltk]))\n",
    "    return lemmas\n",
    "    \n",
    "def cleantextdict_create(textlist,regex,customstops,filelocs,endsplits,topiccleantextfile):\n",
    "    for item in textlist:\n",
    "        k = item.split('_',1)[0].strip()\n",
    "        v = item.split('_',1)[1].strip()\n",
    "        ctext = rmnl(regex.sub('', v).lower().strip())\n",
    "        ctext = re.split(\"|\".join(endsplits),ctext)[0].strip()\n",
    "        ctexts = ctext.split(' ')\n",
    "        cwords = []\n",
    "        for x in ctexts:\n",
    "            # getting rid of stopwords and urls and getting rid of money strings\n",
    "            if x not in customstops and not x.startswith(\"https\"):\n",
    "                if x.isnumeric():\n",
    "                    if int(x) >25000:\n",
    "                        pass\n",
    "                    elif x.endswith(\"000\"):\n",
    "                        pass\n",
    "                    elif len(x) != 8 and x.startswith(tuple([\"416\",\"1888\"])) == False:\n",
    "                        cwords.append(x)\n",
    "                    else:\n",
    "                        continue\n",
    "                # getting rid of postal codes\n",
    "                elif len(x) ==6 and x[0].isalpha() and x[1].isnumeric() and x[2].isalpha() and x[3].isnumeric() and x[4].isalpha() and x[5].isnumeric():\n",
    "                    pass\n",
    "                # getting rid of file numbers\n",
    "                elif x.startswith(tuple(filelocs)) == True and len(x) >=4 and x[3].isnumeric():\n",
    "                    pass\n",
    "                else:\n",
    "                    cwords.append(x)\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        cleantext = ' '.join(cwords)\n",
    "        cleantext = get_lemmas(cleantext)\n",
    "        cleantext = rmnl(re.sub(\"residential tenancy act 17 \",\" rtaa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\"residential tenancy act \",\" rtaa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\"statutory power procedure act rso \",\" sppa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\"statutory power procedure act \",\" sppa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\"tenancy protection act \",\" tpaa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\" protection act \",\" tpaa \",cleantext))\n",
    "        cleantext = rmnl(re.sub(\"x+\\s\",\" \",cleantext))\n",
    "        write_json_tofile({k:cleantext},topiccleantextfile)\n",
    "        \n",
    "def map_filenolocs(x,filelocdict):\n",
    "    cleanjl = []\n",
    "    for k,v in filelocdict.items():\n",
    "        prfxs = [i for i in v if i in x]\n",
    "        if len(prfxs) >0:\n",
    "            cleanjl.append(v[0])\n",
    "        else:\n",
    "            continue\n",
    "    return '|'.join(list(set(cleanjl)))\n",
    "\n",
    "def rmnl(t):\n",
    "    return re.sub('\\s+',' ',t).strip()\n",
    "\n",
    "def sort_collected(row):\n",
    "    collected = row['collected']\n",
    "    caseoutfile = row['caseoutfile']\n",
    "    if collected == 'to_collect':\n",
    "        newcaseoutfile = caseoutfile\n",
    "    else:\n",
    "        newcaseoutfile = collected\n",
    "    return newcaseoutfile\n",
    "\n",
    "def get_filenos(pdffile):\n",
    "    with open(pdffile,\"rb\") as f:\n",
    "        fileReader = PyPDF2.PdfFileReader(f)\n",
    "        pageObj = fileReader.getPage(0)\n",
    "        fileno = pageObj.extractText().split('Ontario Landlord and Tenant Board',1)[0].strip()\n",
    "    return fileno\n",
    "\n",
    "def clean_filenos(x):\n",
    "    if x.startswith('File No.'):\n",
    "        fno = x.split('File No.',1)[1].strip()\n",
    "    elif x.startswith('File Number:'):\n",
    "        fno = x.split('File Number:',1)[1].strip()\n",
    "    elif x.startswith('File Numbers:'):\n",
    "        fno = x.split('File Numbers:',1)[1].strip()\n",
    "    elif x.startswith('SWL'):\n",
    "        fno = x\n",
    "    elif x.startswith('EAL'):\n",
    "        fno = x\n",
    "    else:\n",
    "        fno = 'unknown'\n",
    "    return fno\n",
    "\n",
    "def format_filenos(x):\n",
    "    whitelist = set('abcdefghijklmnopqrstuvwxyz-0123456789 ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    fileno = ''.join(filter(whitelist.__contains__, x))\n",
    "    fileno = rmnl(fileno)\n",
    "    fileno = ' '.join(sorted(fileno.split(' ')))\n",
    "    return fileno\n",
    "\n",
    "def combine_traindata(mainpath,quicklawdict):\n",
    "    # this function matches the annotated QuickLaw cases to those from CanLII\n"
    "    adf1 = pd.read_csv(mainpath + 'output/trainsamples/annotated_cases.csv',sep=',')\n",
    "    del adf1['notes']\n",
    "    adf1.columns = ['qid','applicant','judgementagainst']\n",
    "    adf1['file'] = adf1['qid'].map(quicklawdict)\n",
    "    adf1['file'].fillna('unknown',inplace=True)\n",
    "    print(adf1[adf1['file']=='unknown'].shape)\n",
    "    adf1['fileno'] = adf1['file'].apply(lambda x: get_filenos(x))\n",
    "    adf1['fileno'] = adf1.apply(lambda x: x['fileno'].split(x['qid'],1)[0].strip().rstrip(',').strip(),axis=1)\n",
    "    adf1['fileno'] = adf1['fileno'].apply(lambda x: clean_filenos(x))\n",
    "    print(adf1[adf1['fileno']=='unknown'].shape)\n",
    "    adf1.loc[adf1['fileno'] == 'unknown', ['fileno']] = \"EAL-57778-16, EAT-57251-16, EAT-59980-16\"\n",
    "    adf2 = pd.read_csv(mainpath + 'output/trainsamples/newtrain.csv',sep='\\t')\n",
    "    adf2 = adf2[adf2['changed']==True]\n",
    "    adf2['qid'] = adf2['example'].apply(lambda x: '20'+x.split('Ontario',1)[0].split(', 20',1)[1].strip())\n",
    "    adf2['fileno'] = adf2.apply(lambda x: x['example'].split(x['qid'],1)[0].strip().rstrip(',').strip(),axis=1)\n",
    "    adf2['applicant'] = np.where(adf2['ll_applied']==True,'l','t')\n",
    "    adf2['judgementagainst'] = np.where(adf2['tt_lost']==True,'t','l')\n",
    "    adf2['file'] = adf2['qid'].map(quicklawdict)\n",
    "    adf2['fileno'] = adf2['fileno'].apply(lambda x: re.sub('File No.','',x).strip())\n",
    "    adf2['fileno'] = adf2['fileno'].apply(lambda x: re.sub('File Number:','',x).strip())\n",
    "    adf2['fileno'] = adf2['fileno'].apply(lambda x: re.sub('File Numbers:','',x).strip())\n",
    "    adf2 = adf2[['qid','applicant','judgementagainst','file','fileno']]\n",
    "    adf2['file'].fillna('unknown',inplace=True)\n",
    "    print(adf2[adf2['fileno']=='unknown'].shape)\n",
    "    adf = pd.concat([adf1,adf2])\n",
    "    adf['fileno'] = adf['fileno'].apply(lambda x: format_filenos(x))\n",
    "    return adf\n",
    "\n",
    "def check_forstragglers(row):\n",
    "    ctext = row['topiccleantext']\n",
    "    years = [str(int(row['cid'].split('canlii',1)[0])+1),str(int(row['cid'].split('canlii',1)[0])-1),str(row['cid'].split('canlii',1)[0]),\"2006\",\"1997\",\"1990\",\"2017\",\"17\"]\n",
    "    words = ctext.split(' ')\n",
    "    cleanwords = []\n",
    "    for x in words:\n",
    "        if len(x) ==3 and x[0].isalpha() and x[1].isnumeric() and x[2].isalpha():\n",
    "            pass\n",
    "        elif len(x) ==3 and x[0].isnumeric() and x[1].isalpha() and x[2].isnumeric():\n",
    "            pass\n",
    "        elif len(x) >7 and x[0].isalpha() and x[1].isalpha() and x[2].isalpha() and x[3].isnumeric() and x[4].isnumeric() and x[5].isnumeric() and x[6].isnumeric() and x[7].isnumeric():\n",
    "            pass\n",
    "        elif x in years:\n",
    "            pass\n",
    "        elif x.endswith('00'):\n",
    "            pass\n",
    "        elif x.isalpha() and len(x) <3:\n",
    "            pass\n",
    "        elif x.startswith('0'):\n",
    "            pass\n",
    "        else:\n",
    "            cleanwords.append(x)\n",
    "            continue\n",
    "    return rmnl(' '.join(cleanwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dd0992-4f3a-4b2a-8a91-bd101b2d3974",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bunds/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bunds/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bunds/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bunds/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4)\n",
      "(1, 5)\n",
      "(0, 5)\n",
      "(1734, 5)\n",
      "(1665, 20)\n",
      "(83, 7)\n",
      "nottrainingsample    34857\n",
      "l_l                    803\n",
      "l_t                    335\n",
      "t_t                    333\n",
      "t_l                    171\n",
      "Name: outcome, dtype: int64\n",
      "(36499, 22)\n",
      "(36461, 26)\n",
      "(16817, 27)\n",
      "(3449, 27)\n",
      "(858, 27)\n",
      "TOEast                 6400\n",
      "TOSouth                6335\n",
      "SouthWest_London       4710\n",
      "TONorth                4654\n",
      "Central_Mississauga    4200\n",
      "Southern_Hamilton      3733\n",
      "Eastern_Ottawa         2837\n",
      "Northern_Sudbury       1953\n",
      "missing                 858\n",
      "HeadOffice_Toronto      781\n",
      "Name: newloc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "regex = re.compile('[^a-zA-Z0-9 ]')\n",
    "\n",
    "mainpath = str(os.getcwd())+'/'\n",
    "quicklawfiles = glob.glob(mainpath + 'data/quicklaw/ltb*_*/*.pdf')\n",
    "topiccleantextfile = mainpath + 'output/4_cleantexts_v2.csv'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "quicklawdict = {}\n",
    "for qf in quicklawfiles:\n",
    "    quicklawid = qf.split('/')[-1].split('.pdf')[0].strip()\n",
    "    quicklawdict[quicklawid]=qf\n",
    "\n",
    "df = pd.read_csv(mainpath + 'output/3_metaclean.csv',sep='\\t')\n",
    "adf = combine_traindata(mainpath,quicklawdict)\n",
    "print(adf.shape)\n",
    "\n",
    "df['fileno_clean'] = df['fileno'].apply(lambda x: re.sub('\\(Re\\)','',x).strip())\n",
    "adf['fileno_clean'] = adf['fileno'].apply(lambda x: re.sub('\\(Re\\)','',x).strip())\n",
    "print(df[df['fileno_clean'].isin(adf['fileno'].tolist())].shape)\n",
    "adf['cid'] = adf['fileno'].map(dict(zip(df['fileno_clean'],df['cid'])))\n",
    "adf['cid'].fillna('quicklaw_nomatch_tocanlii',inplace=True)\n",
    "print(adf[adf['cid']=='quicklaw_nomatch_tocanlii'].shape)\n",
    "\n",
    "adf['winner'] = adf['judgementagainst'].map({'t':'l','l':'t'})\n",
    "adf['outcome'] = adf['applicant'] + '_' + adf['winner']\n",
    "adf = adf[adf['cid']!= 'quicklaw_nomatch_tocanlii']\n",
    "df['outcome'] = df['cid'].map(dict(zip(adf['cid'],adf['outcome'])))\n",
    "df['outcome'].fillna('nottrainingsample',inplace=True)\n",
    "print(df['outcome'].value_counts())\n",
    "\n",
    "filelocdict = {\"Central_Mississauga\":['CEL','CET'],\"Northern_Sudbury\":['NOL','NOT'],\"Southern_Hamilton\":['SOL','SOT'],\"SouthWest_London\":['SWL','SWT'],\"Eastern_Ottawa\":['EAL','EAT'],\"TOEast\":['TEL','TET'],\"TONorth\":['TNL','TNT'],\"TOSouth\":['TSL','TST']}\n",
    "\n",
    "reversefilelocdict = {}\n",
    "for k,v in filelocdict.items():\n",
    "    reversefilelocdict[v[0]]=k\n",
    "filelocs = [item.lower() for sublist in list(filelocdict.values()) for item in sublist]\n",
    "\n",
    "df['cleantext'] = df['cleantext'].apply(lambda x: x.split('canlii_',1)[1].strip())\n",
    "df['cleantext'] = df['cleantext'].apply(lambda x: regex.sub('', x))\n",
    "df['idxtext'] = df.apply(lambda x: str(x['cid']) + '_' + x['otext'],axis=1)\n",
    "print(df.shape)\n",
    "df['appcleantext'] = df['cleantext'].apply(lambda x: ' '.join(x.split(' ')[:100]))\n",
    "df['wincleantext'] = df['cleantext'].apply(lambda x: ' '.join(x.split(' ')[-round(0.5*len(x.split(' '))):]).strip())\n",
    "df = df[df['errtype']=='no_error']# 38 cases with bad links\n",
    "df['applicant'] = df['outcome'].apply(lambda x: x.split('_')[0])\n",
    "df['winner'] = df['outcome'].apply(lambda x: x.split('_')[-1])\n",
    "print(df.shape)\n",
    "\n",
    "# replacing the locations for 2 of the cases:\n",
    "df.loc[df.cid == \"YEARcanliiIDNUMBER\", \"loc\"] = \"TOSouth\"\n",
    "df.loc[df.cid == \"YEARcanliiIDNUMBER\", \"loc\"] = \"TOSouth\"\n",
    "\n",
    "df['filenoloc'] = df['fileno'].apply(lambda x: map_filenolocs(x,filelocdict))\n",
    "df['filenoloc'] = df['filenoloc'].map(reversefilelocdict)\n",
    "df.replace(r'^\\s*$', np.nan, regex=True,inplace=True)\n",
    "df.fillna('missing',inplace=True)\n",
    "print(df[df['filenoloc']=='missing'].shape)\n",
    "print(df[df['loc']=='missing'].shape)\n",
    "print(df[(df['filenoloc']=='missing')&(df['loc']=='missing')].shape)\n",
    "\n",
    "df['newloc'] = np.where(df['loc']=='missing',df['filenoloc'],df['loc'])\n",
    "print(df['newloc'].value_counts())\n",
    "# df[(df['filenoloc']!=df['loc'])&(df['filenoloc']!='missing')&(df['loc']!='missing')]#['text'].tolist()[0]\n",
    "# df[(df['filenoloc']!=df['loc'])&(df['filenoloc']!='missing')&(df['loc']!='missing')]['text'].tolist()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb5bdf4-da99-47dd-b3f2-1e2002d50133",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = ' '.join(list(set([x for x in df['member'].tolist() if x != 'missing'])))\n",
    "members = rmnl(regex.sub(' ', members).lower().strip()).split(' ')\n",
    "canliiids = [str(x.split('canlii',1)[1]) for x in df['cid'].tolist()]\n",
    "filenolist = list(set([rmnl(regex.sub('', re.sub('(Re)',' ',x)).lower().strip()) for x in df['fileno'].tolist()])) \n",
    "filenolist = list(set([item for sublist in [x.split() for x in filenolist] for item in sublist]))\n",
    "monthnames = [x.lower() for x in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August','September','October', 'November','December','Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct','Nov','Dec', 'Sept','Sep']]\n",
    "stopwordslist = [x.lower() for x in list(get_stop_words('en')) + list(stopwords.words('english'))] + [\"summary\",\"commentary\",\"from\",\"the\",\"legal\",\"community\",\"available\",\"add\",\"your\",\"own\",\"order\",\"under\",'file',\"if\",\"you\",\"have\",\"any\",\"questions\",\"about\",\"this\",\"contact\",\"ontario\",\"file\",\"citation\",\"citations\",\"canlii\",\"document\",\"history\",\"loading\", \"paragraph\",\"treatment\",\"markers\",\"pdf\",\"date\",\"number\",\"cited\",\"documents\",\"home\",\"board\",\"landlord\",\"tenant\",\"page\",\"ltb\",\"connects\",\"date\",\"retrieved\",\"20220108\",\"legislation\",\"discussions\", \"unfavourable\", \"mentions\", \"expanded\", \"collapsed\", \"summaries\",\"retrieved\",\"0\", \"back\", \"top\", \"directors\", \"feedback\", \"information\", \"terms\", \"use\", \"privacy\", \"policy\", \"primary\", \"law\", \"databases\", \"authors\", \"program\", \"tools\", \"rss\", \"feeds\", \"apis\", \"lexbox\", \"help\", \"frequently\", \"asked\", \"search\", \"reflex\", \"citator\", \"guides\", \"follow\", \"us\", \"darklii\", \"onoff\",\"23546\",\"call\", \"toll\", \"free\", \"reasons\", \"issued\",\"member\",\"north\",\"south\",\"east\",\"west\"] \n",
    "customstopslist = monthnames + stopwordslist + filenolist +canliiids+ members\n",
    "customstopslemmas = [get_lemmas(j.lower()) for j in customstopslist]\n",
    "customstops = list(set(customstopslist + customstopslemmas))\n",
    "print(len(customstops))\n",
    "print(customstops[:2])\n",
    "endsplits = [\"centralro\",\"northernro\",\"southernro\",\"westro\",\"easternro\",\"eastro\",\"northro\",\"southro\",\"head office\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f1e2f2-100a-4f45-9b40-84b6dc560316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36461\n",
      "730\n"
     ]
    }
   ],
   "source": [
    "texts = df['idxtext'].tolist()\n",
    "print(len(texts))\n",
    "textchunks = list(divide_chunks(texts,50))\n",
    "print(len(textchunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fb9d-a24b-42a6-911c-d0542ea20355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleantextdict_create(textchunks[0],regex,customstops,filelocs,endsplits,topiccleantextfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a3fb48-1996-4dfc-b77b-d484b29f625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 730/730 [1:00:26<00:00,  1.54s/it]"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(processes=10)\n",
    "pbar = tqdm(textchunks)\n",
    "for i in range(pbar.total): \n",
    "    pool.apply_async(cleantextdict_create, args=(textchunks[i],regex,customstops,filelocs,endsplits,topiccleantextfile), callback=update)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c175585-cbbb-4052-9b51-ecfa9e431e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctextdict = {}\n",
    "ctextdicts = read_jsoncsv(topiccleantextfile)\n",
    "for item in ctextdicts:\n",
    "    for k,v in item.items():\n",
    "        ctextdict[k] = v\n",
    "print(len(ctextdicts))\n",
    "print(ctextdicts[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54192f-9ae4-4f61-9135-2a8266722523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if there are some cases not processed, uncomment and run the following cell\"\"\"\n",
    "# missingcids = list(set(df['cid'].tolist())-set(list(ctextdict.keys())))\n",
    "# print(len(missingcids))\n",
    "# missing = [x for x in texts if x.split('_',1)[0] in missingcids]\n",
    "# print(len(missing))\n",
    "\n",
    "# cleantextdict_create(missing,regex,customstops,topiccleantextfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89657fca-a21a-41fb-8e81-6bd572edc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topiccleantext'] = df['cid'].map(ctextdict)\n",
    "df['topiccleantext'] = df.apply(lambda x: check_forstragglers(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7439b411-b78c-4c4d-adf2-ef46ffe8ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random.sample(df['topiccleantext'].tolist(),1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c42910-8d98-4bce-85de-4876517a3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 29)\n",
      "(0, 29)\n"
     ]
    }
   ],
   "source": [
    "df.fillna('missing',inplace=True)\n",
    "df['cleantext'] = df['cleantext'].apply(lambda x: rmnl(re.sub(\"xx+\\s\",\" \",x)))\n",
    "print(df[df['topiccleantext']=='missing'].shape)\n",
    "print(df[df['cleantext']=='missing'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12d9e937-5036-4c86-a8e3-fdfeecf94ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(mainpath + 'output/4_traindataplus.csv',sep='\\t',index=False,quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
