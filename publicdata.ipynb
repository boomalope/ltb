{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0e6123-9677-4fec-8704-dc5958bcf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import scipy.sparse as ss\n",
    "from htmlparsing_kit import clean_sections\n",
    "\n",
    "def clean_members(x,mdict):\n",
    "    judges = []\n",
    "    for item in x.split(\"_\"):\n",
    "        item = item.strip()\n",
    "        if item in list(mdict.keys()):\n",
    "            judges.append(str(mdict[item]))\n",
    "        else:\n",
    "            continue\n",
    "    judges = '_'.join(list(set(judges)))\n",
    "    return judges \n",
    "\n",
    "def clean_sections2(x):\n",
    "    x = str(x)\n",
    "    if len(x) <100:\n",
    "        newc = x\n",
    "    else:\n",
    "        newc=''\n",
    "    if newc == 'nan':\n",
    "        newc = ''\n",
    "    else:\n",
    "        newc = x\n",
    "    return newc\n",
    "\n",
    "def map_vectwords(x,words):\n",
    "    pwords = []\n",
    "    for w in words:\n",
    "        if w in x:\n",
    "            pwords.append(w)\n",
    "        else:\n",
    "            continue\n",
    "    pwords = sorted(list(set(pwords)))\n",
    "    return ', '.join(pwords)\n",
    "\n",
    "def keywordstotext(row):\n",
    "    kws = row['keywords'].split(',')\n",
    "    t = row['topiccleantext']\n",
    "    torderdict = {}\n",
    "    for kw in kws:\n",
    "        kw = str(kw).strip()\n",
    "        if t.find(kw) != -1:\n",
    "            torderdict[t.find(kw)] = kw\n",
    "        else:\n",
    "            continue\n",
    "    torderdict = dict(sorted(torderdict.items()))\n",
    "    kwordered = ' '.join(list(torderdict.values()))\n",
    "    return kwordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba7819c-44e3-4f81-b4e7-8af4195dbc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36461, 35)\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "mainpath = str(os.getcwd())+'/'\n",
    "\n",
    "df = pd.read_csv(mainpath+\"output/5_model_results.csv\",sep='\\t')\n",
    "df['cleansections']=df['sections'].apply(lambda x: clean_sections(x))\n",
    "df['datetime'] = pd.to_datetime(df['date'])\n",
    "df['year'] =df['datetime'].dt.year\n",
    "df['quarter'] = pd.PeriodIndex(df.datetime, freq='Q')\n",
    "df['monthyear'] = pd.PeriodIndex(df.datetime, freq='M')\n",
    "df['numone'] = 1\n",
    "print(df.shape)\n",
    "df.fillna(np.nan,inplace=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df['id'] = list(range(0,df.shape[0]))\n",
    "\n",
    "members = [x for x in list(set([x.strip() for x in '_'.join(list(set(df['member'].tolist()))).split('_')])) if 'ottawa' not in x.lower()]\n",
    "random.shuffle(members)\n",
    "mdict = dict(zip(members,list(range(0,len(members)))))\n",
    "print(len(mdict))\n",
    "df['cleanmember'] = df['member'].apply(lambda x:clean_members(x,mdict))\n",
    "df['csections'] = df['cleansections'].apply(lambda x: clean_sections2(x))\n",
    "\n",
    "docs = df['topiccleantext'].tolist()\n",
    "vectorizern = TfidfVectorizer(ngram_range=(1, 1), max_features = 6000, max_df = 0.95, min_df = 0.05)\n",
    "dtm_tf = vectorizern.fit_transform(docs)\n",
    "dtm_tf = ss.csr_matrix(dtm_tf)\n",
    "words = vectorizern.get_feature_names_out()\n",
    "\n",
    "df['keywords'] = df['topiccleantext'].apply(lambda x: map_vectwords(x,words))\n",
    "df['dbtext'] = df.apply(lambda x: keywordstotext(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddabf1a-7988-4247-8fbc-dbd0e6622ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df[['year','id','date','newloc','cleanmember','csections','applicant','winner','dbtext']]\n",
    "pdf.columns = ['year','Case Id','Date','Office Location','Adjudicators','Act Sections','Applicant','Winner','Order Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed188706-1122-4ef9-8519-78ef0d65e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,g in pdf.groupby('year'):\n",
    "    publicdbpath = mainpath + 'output/publicdb/onltbdb_'+str(n)+'.csv'\n",
    "    # print(\"csvtotable \"+ publicdbpath.split('/')[-1] + \" --caption \" + '\"Ontario LTB Orders ('+str(n)+')\" '+ publicdbpath.split('/')[-1].split('.')[0].strip()+'.html')\n",
    "    g = g[['Case Id','Date','Office Location','Adjudicators','Act Sections','Applicant','Winner','Order Text']]\n",
    "    g.to_csv(publicdbpath,sep=',',index=False,quoting=csv.QUOTE_ALL)\n",
    "# example for converting csvs to searchable html tables:\n",
    "# csvtotable onltbdb_2006.csv --caption \"Ontario LTB Orders (2006)\" onltbdb_2006.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fd4dfa-0f20-4f6c-b21d-ef4bf9e6c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_csv(mainpath + 'output/onltbdb_2006_2021.csv',sep=',',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
